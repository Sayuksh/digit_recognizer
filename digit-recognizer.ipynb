{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy\nimport numpy as np\nimport pandas as pd \nimport torch \nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T19:16:59.729148Z","iopub.execute_input":"2023-09-18T19:16:59.729568Z","iopub.status.idle":"2023-09-18T19:17:11.492206Z","shell.execute_reply.started":"2023-09-18T19:16:59.729537Z","shell.execute_reply":"2023-09-18T19:17:11.490841Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:11.495110Z","iopub.execute_input":"2023-09-18T19:17:11.495519Z","iopub.status.idle":"2023-09-18T19:17:16.372748Z","shell.execute_reply.started":"2023-09-18T19:17:11.495478Z","shell.execute_reply":"2023-09-18T19:17:16.371650Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:16.374352Z","iopub.execute_input":"2023-09-18T19:17:16.374849Z","iopub.status.idle":"2023-09-18T19:17:16.402107Z","shell.execute_reply.started":"2023-09-18T19:17:16.374812Z","shell.execute_reply":"2023-09-18T19:17:16.401019Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test=test/255","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:16.404875Z","iopub.execute_input":"2023-09-18T19:17:16.405362Z","iopub.status.idle":"2023-09-18T19:17:16.487779Z","shell.execute_reply.started":"2023-09-18T19:17:16.405322Z","shell.execute_reply":"2023-09-18T19:17:16.486740Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"Y_train  = train.label.values\nX_train = train.loc[:,train.columns != \"label\"].values/255 # normalization","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:16.489134Z","iopub.execute_input":"2023-09-18T19:17:16.489505Z","iopub.status.idle":"2023-09-18T19:17:16.685464Z","shell.execute_reply.started":"2023-09-18T19:17:16.489471Z","shell.execute_reply":"2023-09-18T19:17:16.684447Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"X_test=test.values\nX_train = X_train.reshape(-1, 28, 28)\nX_test=X_test.reshape(-1,28,28)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:16.687140Z","iopub.execute_input":"2023-09-18T19:17:16.687878Z","iopub.status.idle":"2023-09-18T19:17:16.700579Z","shell.execute_reply.started":"2023-09-18T19:17:16.687841Z","shell.execute_reply":"2023-09-18T19:17:16.699407Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X_train,\n                                                  Y_train,\n                                                  test_size = 0.1,\n                                                 random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:16.701921Z","iopub.execute_input":"2023-09-18T19:17:16.703270Z","iopub.status.idle":"2023-09-18T19:17:17.091849Z","shell.execute_reply.started":"2023-09-18T19:17:16.703234Z","shell.execute_reply":"2023-09-18T19:17:17.090765Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[20].reshape(28,28).shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.093688Z","iopub.execute_input":"2023-09-18T19:17:17.094094Z","iopub.status.idle":"2023-09-18T19:17:17.102371Z","shell.execute_reply.started":"2023-09-18T19:17:17.094058Z","shell.execute_reply":"2023-09-18T19:17:17.101164Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"(28, 28)"},"metadata":{}}]},{"cell_type":"code","source":"plt.imshow(X_train[21].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(Y_train[21]))\nplt.savefig('graph.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.104191Z","iopub.execute_input":"2023-09-18T19:17:17.105101Z","iopub.status.idle":"2023-09-18T19:17:17.336547Z","shell.execute_reply.started":"2023-09-18T19:17:17.105065Z","shell.execute_reply":"2023-09-18T19:17:17.335396Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"class dig(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 512, kernel_size=7, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv2 = nn.Conv2d(512, 256, kernel_size=5, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv3=nn.Conv2d(256,64,kernel_size=3,padding=1)\n        self.relu3=nn.ReLU()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(5*5*64, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            \n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            \n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            \n            nn.Linear(64, 32),\n            nn.ReLU(),\n            \n            nn.Linear(32, 10),\n            nn.LogSoftmax(dim=1)\n        )\n    def forward(self,input):\n        input=input.to(torch.float32)\n        #print(input.shape)\n        output=self.conv1(input)\n        output=self.relu1(output)\n        output=self.pool1(output)\n        \n        output=self.conv2(output)\n        output=self.relu2(output)\n        output=self.pool2(output)\n        \n        output=self.conv3(output)\n        output=self.relu3(output)\n        \n        \n        output=output.view(output.size(0), -1)\n        \n        output=self.classifier(output)\n        return output\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.340778Z","iopub.execute_input":"2023-09-18T19:17:17.341125Z","iopub.status.idle":"2023-09-18T19:17:17.354839Z","shell.execute_reply.started":"2023-09-18T19:17:17.341096Z","shell.execute_reply":"2023-09-18T19:17:17.353273Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"X_train = torch.FloatTensor(X_train)\nX_val = torch.FloatTensor(X_val)\nY_train = torch.tensor(Y_train, dtype=torch.long)\nY_val = torch.tensor(Y_val, dtype=torch.long)\nX_test = torch.FloatTensor(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.359244Z","iopub.execute_input":"2023-09-18T19:17:17.359731Z","iopub.status.idle":"2023-09-18T19:17:17.539538Z","shell.execute_reply.started":"2023-09-18T19:17:17.359693Z","shell.execute_reply":"2023-09-18T19:17:17.538507Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model=dig().to('cuda')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.540822Z","iopub.execute_input":"2023-09-18T19:17:17.541179Z","iopub.status.idle":"2023-09-18T19:17:17.584783Z","shell.execute_reply.started":"2023-09-18T19:17:17.541147Z","shell.execute_reply":"2023-09-18T19:17:17.583613Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"optimiser=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\nloss_function=nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.586568Z","iopub.execute_input":"2023-09-18T19:17:17.586917Z","iopub.status.idle":"2023-09-18T19:17:17.594822Z","shell.execute_reply.started":"2023-09-18T19:17:17.586883Z","shell.execute_reply":"2023-09-18T19:17:17.593665Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"train_count=42000*0.8\ntest_count=42000*0.2","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:17:17.596789Z","iopub.execute_input":"2023-09-18T19:17:17.597396Z","iopub.status.idle":"2023-09-18T19:17:17.604930Z","shell.execute_reply.started":"2023-09-18T19:17:17.597339Z","shell.execute_reply":"2023-09-18T19:17:17.603863Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nbatch_size = 128\nbest_accuracy=0\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for i in range(0, len(X_train), batch_size):\n        inputs = X_train[i:i+batch_size].unsqueeze(1).to('cuda')\n        labels = Y_train[i:i+batch_size].to('cuda')\n        optimiser.zero_grad()\n        outputs = model(inputs)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimiser.step()\n        train_loss += loss.item()\n    \n    model.eval()\n    val_correct = 0\n    val_total = 0\n    val_loss = 0.0\n    with torch.no_grad():\n        for i in range(0, len(X_val), batch_size):\n            inputs = X_val[i:i+batch_size].unsqueeze(1).to('cuda')\n            labels = Y_val[i:i+batch_size].to('cuda')\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    if (epoch+1%10==1):\n        print(f\"Epoch {epoch+1} Train Loss: {(train_loss/len(X_train)):.4f} Validation Loss: {(val_loss/len(X_val)):.4f} Validation Accuracy: {(val_correct/val_total):.4f}\")\n    if val_correct/val_total>best_accuracy:\n        torch.save(model.state_dict(),'best_checkpoint.model')\n        best_accuracy=val_correct/val_total\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:20:37.534542Z","iopub.execute_input":"2023-09-18T19:20:37.535842Z","iopub.status.idle":"2023-09-18T19:42:32.722277Z","shell.execute_reply.started":"2023-09-18T19:20:37.535801Z","shell.execute_reply":"2023-09-18T19:42:32.721222Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch 1 Train Loss: 0.0002 Validation Loss: 0.0003 Validation Accuracy: 0.9893\nEpoch 2 Train Loss: 0.0002 Validation Loss: 0.0003 Validation Accuracy: 0.9895\nEpoch 3 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9898\nEpoch 4 Train Loss: 0.0002 Validation Loss: 0.0003 Validation Accuracy: 0.9900\nEpoch 5 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9888\nEpoch 6 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9898\nEpoch 7 Train Loss: 0.0002 Validation Loss: 0.0003 Validation Accuracy: 0.9917\nEpoch 8 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9893\nEpoch 9 Train Loss: 0.0002 Validation Loss: 0.0005 Validation Accuracy: 0.9888\nEpoch 10 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9881\nEpoch 11 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9886\nEpoch 12 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9881\nEpoch 13 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9912\nEpoch 14 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9917\nEpoch 15 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9914\nEpoch 16 Train Loss: 0.0002 Validation Loss: 0.0004 Validation Accuracy: 0.9907\nEpoch 17 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9912\nEpoch 18 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9876\nEpoch 19 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9926\nEpoch 20 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9921\nEpoch 21 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9893\nEpoch 22 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9924\nEpoch 23 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9893\nEpoch 24 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9893\nEpoch 25 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9886\nEpoch 26 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9883\nEpoch 27 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9883\nEpoch 28 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9921\nEpoch 29 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9898\nEpoch 30 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9910\nEpoch 31 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9893\nEpoch 32 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9910\nEpoch 33 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9879\nEpoch 34 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9881\nEpoch 35 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9902\nEpoch 36 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9902\nEpoch 37 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9912\nEpoch 38 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9910\nEpoch 39 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9890\nEpoch 40 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9914\nEpoch 41 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9914\nEpoch 42 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9907\nEpoch 43 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9926\nEpoch 44 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9914\nEpoch 45 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9921\nEpoch 46 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9910\nEpoch 47 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9910\nEpoch 48 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9881\nEpoch 49 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9905\nEpoch 50 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9900\nEpoch 51 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9924\nEpoch 52 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9912\nEpoch 53 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9898\nEpoch 54 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9917\nEpoch 55 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9879\nEpoch 56 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9902\nEpoch 57 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9936\nEpoch 58 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9910\nEpoch 59 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9921\nEpoch 60 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9905\nEpoch 61 Train Loss: 0.0000 Validation Loss: 0.0005 Validation Accuracy: 0.9898\nEpoch 62 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9919\nEpoch 63 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9881\nEpoch 64 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9910\nEpoch 65 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9919\nEpoch 66 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9902\nEpoch 67 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9910\nEpoch 68 Train Loss: 0.0001 Validation Loss: 0.0007 Validation Accuracy: 0.9855\nEpoch 70 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9910\nEpoch 71 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9895\nEpoch 72 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9905\nEpoch 73 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9883\nEpoch 74 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9912\nEpoch 75 Train Loss: 0.0001 Validation Loss: 0.0006 Validation Accuracy: 0.9895\nEpoch 76 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9895\nEpoch 77 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9910\nEpoch 78 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9886\nEpoch 79 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9900\nEpoch 80 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9898\nEpoch 81 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9917\nEpoch 82 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9924\nEpoch 83 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9886\nEpoch 84 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9919\nEpoch 85 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9912\nEpoch 86 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9917\nEpoch 87 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9912\nEpoch 88 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9924\nEpoch 89 Train Loss: 0.0001 Validation Loss: 0.0005 Validation Accuracy: 0.9893\nEpoch 90 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9926\nEpoch 91 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9919\nEpoch 92 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9914\nEpoch 93 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9898\nEpoch 94 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9917\nEpoch 95 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9924\nEpoch 97 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9902\nEpoch 98 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9905\nEpoch 99 Train Loss: 0.0001 Validation Loss: 0.0003 Validation Accuracy: 0.9921\nEpoch 100 Train Loss: 0.0001 Validation Loss: 0.0004 Validation Accuracy: 0.9921\n","output_type":"stream"}]},{"cell_type":"code","source":"best_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:42:32.724266Z","iopub.execute_input":"2023-09-18T19:42:32.727209Z","iopub.status.idle":"2023-09-18T19:42:32.733826Z","shell.execute_reply.started":"2023-09-18T19:42:32.727179Z","shell.execute_reply":"2023-09-18T19:42:32.732740Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"0.9935714285714285"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint=torch.load('/kaggle/working/best_checkpoint.model')\nmodel=dig().to('cuda')\nmodel.load_state_dict(checkpoint)\nmodel.eval()\npredictions = []\n\nmodel.eval()\n\nwith torch.no_grad():\n    for i in range(0, len(X_test), batch_size):\n        batch = X_test[i:i+batch_size].unsqueeze(1).to('cuda')\n        outputs = model(batch)\n        _,predicted = torch.max(outputs.cpu(), 1)\n        predictions.extend(predicted.numpy())\n    \nsubmission_df = pd.DataFrame({\"ImageId\": range(1, len(X_test) + 1), \"Label\": predictions})\nsubmission_df.to_csv(\"submission_base.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T19:48:16.315985Z","iopub.execute_input":"2023-09-18T19:48:16.316373Z","iopub.status.idle":"2023-09-18T19:48:19.834664Z","shell.execute_reply.started":"2023-09-18T19:48:16.316342Z","shell.execute_reply":"2023-09-18T19:48:19.833471Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}